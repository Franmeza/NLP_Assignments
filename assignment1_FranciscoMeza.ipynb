{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a59c7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ead86b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\57317\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\57317\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\57317\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ceb166",
   "metadata": {},
   "source": [
    "## 1. Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e701125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv(\"./data/amazon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae419069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text length statistics:\n",
      "count    19996.000000\n",
      "mean       175.787257\n",
      "std         58.964840\n",
      "min          3.000000\n",
      "25%        123.000000\n",
      "50%        164.000000\n",
      "75%        254.000000\n",
      "max        254.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Add column for lenght of reviews \n",
    "df['text_length'] = df['Text'].apply(len)\n",
    "print(\"\\nText length statistics:\")\n",
    "print(df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42b93b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (19996, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19996 entries, 0 to 19995\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Text         19996 non-null  object\n",
      " 1   label        19996 non-null  int64 \n",
      " 2   text_length  19996 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 468.8+ KB\n",
      "None\n",
      "\n",
      "Sample rows:\n",
      "                                                Text  label  text_length\n",
      "0  This is  the best apps acording to a bunch of ...      1          121\n",
      "1  This is a pretty good version of the game for ...      1          129\n",
      "2  this is a really . there are a bunch of levels...      1           87\n",
      "3  This is a silly game and can be frustrating, b...      1          105\n",
      "4  This is a terrific game on any pad. Hrs of fun...      1          117\n"
     ]
    }
   ],
   "source": [
    "# Dataset shape and info\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"\\nSample rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a472b",
   "metadata": {},
   "source": [
    "## 2. Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f08690",
   "metadata": {},
   "source": [
    "**Importance of lowercasing for text preprocesing:**\n",
    "\n",
    "Lowercasing unifies words so it does not interpretate ones that means the same as different because of caps among them.\n",
    "\n",
    "By lowercasing words will be treated consistently. As a result, it will improve quality, accuracy and efficienty of text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45548c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowercased sample:\n",
      "0    this is  the best apps acording to a bunch of ...\n",
      "1    this is a pretty good version of the game for ...\n",
      "2    this is a really . there are a bunch of levels...\n",
      "3    this is a silly game and can be frustrating, b...\n",
      "4    this is a terrific game on any pad. hrs of fun...\n",
      "Name: clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# a) Convert to lowercase\n",
    "df['clean'] = df['Text'].str.lower()\n",
    "print(\"\\nLowercased sample:\")\n",
    "print(df['clean'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e256a1",
   "metadata": {},
   "source": [
    "**Trade-offs of punctuation removal:**\n",
    "\n",
    "Punctuation doesn't usually carries meaning. Removing them simplifies the text and makes it cleaner whichs makes it easier to process. It also improves consistency during tokenization and reduces vocabulary size and punctuation attached to words creates duplicates.\n",
    "\n",
    "However, loss of emotional, contextual information or grammatical structure may occurs since punctuation can express strong sentiment, emphasis and sentence structure.\n",
    "\n",
    "Wheater removing punctuation depends on the goal. For simple frequency analysis or classification, it is fine doing. But if tone, emotion or sentence structure are important, certain punctuation marks might be kept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d508970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After punctuation removal:\n",
      "0    this is  the best apps acording to a bunch of ...\n",
      "1    this is a pretty good version of the game for ...\n",
      "2    this is a really  there are a bunch of levels ...\n",
      "3    this is a silly game and can be frustrating bu...\n",
      "4    this is a terrific game on any pad hrs of fun ...\n",
      "Name: clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# b) Remove punctuation\n",
    "def remove_punct(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "df['clean'] = df['clean'].apply(remove_punct)\n",
    "print(\"\\nAfter punctuation removal:\")\n",
    "print(df['clean'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331d9cc",
   "metadata": {},
   "source": [
    "**Stop words removal:**\n",
    "Stopwords are common words in a language that appear very frequently but carry little meaningful information on their own.\n",
    "\n",
    "These words are essential for grammar, but usually do not help distinguish topics, sentiment, or meaning in text analysis.\n",
    "\n",
    "Removing stop words reduces noise in the data, decrease vocabulary size, improve model focus on meaningful words and simplify text for tasks such as as tokenization, vectorization or text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77ccf771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After stopword removal:\n",
      "0    best apps acording bunch people agree bombs eg...\n",
      "1    pretty good version game free lots different l...\n",
      "2       really bunch levels find golden eggs super fun\n",
      "3    silly game frustrating lots fun definitely rec...\n",
      "4    terrific game pad hrs fun grandkids love great...\n",
      "Name: clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# c) Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([w for w in text.split() if w not in stop_words])\n",
    "\n",
    "df['clean'] = df['clean'].apply(remove_stopwords)\n",
    "print(\"\\nAfter stopword removal:\")\n",
    "print(df['clean'].head())\n",
    "# Rationale: Stopwords carry little semantic meaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22885be",
   "metadata": {},
   "source": [
    "**Tokenization:**\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units, called tokens. Tokens are usually words, but they can also be subwords, characters, or sentences depending on the task.\n",
    "\n",
    "It enables analysis at the word lever, prepares text for preprocessing steps, reduces complexity for machine learning algorithms and helps in handling punctuation and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b68b9815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized sample:\n",
      "0    [best, apps, acording, bunch, people, agree, b...\n",
      "1    [pretty, good, version, game, free, lots, diff...\n",
      "2    [really, bunch, levels, find, golden, eggs, su...\n",
      "3    [silly, game, frustrating, lots, fun, definite...\n",
      "4    [terrific, game, pad, hrs, fun, grandkids, lov...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# d) Tokenization\n",
    "df['tokens'] = df['clean'].apply(word_tokenize)\n",
    "print(\"\\nTokenized sample:\")\n",
    "print(df['tokens'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568a8f4",
   "metadata": {},
   "source": [
    "## 3.Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f3cae",
   "metadata": {},
   "source": [
    "**Lemmatization:**\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or dictionary form, considering context and part of speech (e.g., \"running\" → \"run\", \"better\" → \"good\"). \n",
    "\n",
    "Its main advantage is that it produces real, meaningful words, reduces vocabulary redundancy, and improves the consistency of NLP tasks like sentiment analysis or topic modeling. \n",
    "\n",
    "However, it is slower than stemming, depends on accurate part-of-speech tagging, may not handle slang or misspellings, and requires external libraries or dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfb7661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After lemmatization:\n",
      "0    [best, apps, acording, bunch, people, agree, b...\n",
      "1    [pretty, good, version, game, free, lot, diffe...\n",
      "2    [really, bunch, level, find, golden, egg, supe...\n",
      "3    [silly, game, frustrating, lot, fun, definitel...\n",
      "4    [terrific, game, pad, hr, fun, grandkids, love...\n",
      "Name: lemmas, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# a) Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmas'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(w) for w in tokens])\n",
    "print(\"\\nAfter lemmatization:\")\n",
    "print(df['lemmas'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda5208",
   "metadata": {},
   "source": [
    "**Handling emojis and slangs:**\n",
    "\n",
    "Handling emojis and slang in text data can be approached in different ways. Each of them has pros and cons.\n",
    "\n",
    "**Pros and cons:**\n",
    "\n",
    "**Replacing with descriptive words:** Preserves meaning and sentiment, but requires comprehensive mappings and may not cover all cases.\n",
    "\n",
    "**Ignoring them:** Easy to implement and reduces noise, but valuable information about emotion or emphasis is lost.\n",
    "\n",
    "**Using libraries:** Automates handling and covers many cases, but adds dependencies and may still miss rare or creative expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "101deff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After handling slang:\n",
      "0    [best, apps, acording, bunch, people, agree, b...\n",
      "1    [pretty, good, version, game, free, lot, diffe...\n",
      "2    [really, bunch, level, find, golden, egg, supe...\n",
      "3    [silly, game, frustrating, lot, fun, definitel...\n",
      "4    [terrific, game, pad, hour, fun, grandkids, lo...\n",
      "Name: normalized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# b) Handling slangs\n",
    "words_mapping ={\n",
    "    \"hrs\":\"hours\",\n",
    "    \"hr\":\"hour\",\n",
    "    \"&\":\"and\",\n",
    "    \"ad\":\"advertising\",\n",
    "    \"omg\":\"oh my god\",\n",
    "    \"fav\": \"favorite\"\n",
    "}\n",
    "\n",
    "def norm_text(tok_list):\n",
    "    return [ words_mapping.get(w, w) for w in tok_list]\n",
    "\n",
    "df['normalized'] = df['lemmas'].apply(norm_text)\n",
    "print(\"\\nAfter handling slang:\")\n",
    "print(df['normalized'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05792a7",
   "metadata": {},
   "source": [
    "**Handling Numbers:**\n",
    "\n",
    "I selected the approach to convert numbers to text as it preserves the semantic meaning of numerical information, which is often important in product reviews for ratings, quantities, or measurements. In this particular case, some reviews mention ages.\n",
    "\n",
    "Converting numbers to words allows NLP models to process them as meaningful tokens rather than isolated digits, improving consistency and interpretability during text analysis. Keeping numbers as digits could cause models to treat them differently from words, while removing them would result in loss of information. \n",
    "\n",
    "**Note:** num2words library was used for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b90530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    best apps acording bunch people agree bomb egg...\n",
      "1    pretty good version game free lot different le...\n",
      "2         really bunch level find golden egg super fun\n",
      "3    silly game frustrating lot fun definitely reco...\n",
      "4    terrific game pad hour fun grandkids love grea...\n",
      "Name: final_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# c) Handling numbers\n",
    "def numbers_to_words(words_list):\n",
    "    new_tokens = []\n",
    "    for token in words_list:\n",
    "        if token.isdigit():  # check if token is a number\n",
    "            new_tokens.append(num2words(int(token)))\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "# Apply to the 'normalized' column\n",
    "df['normalized_numbers'] = df['normalized'].apply(numbers_to_words)\n",
    "\n",
    "# Combine tokens into final cleaned text\n",
    "df['final_text'] = df['normalized_numbers'].apply(lambda lst: ' '.join(lst))\n",
    "\n",
    "# Display sample\n",
    "print(df['final_text'].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignments_nlp (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
